{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'readppt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 36\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m DataLoader\n\u001b[1;32m     34\u001b[0m tokenizer \u001b[39m=\u001b[39m GPT2Tokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mgpt2\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 36\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mreadppt\u001b[39;00m \u001b[39mimport\u001b[39;00m read_ppt\n\u001b[1;32m     37\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39murlscrape\u001b[39;00m\n\u001b[1;32m     39\u001b[0m load_dotenv()\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'readppt'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import nltk\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import concurrent.futures\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "from functools import lru_cache\n",
    "from typing import List, Tuple, Dict\n",
    "from urllib.parse import urlparse, urljoin\n",
    "\n",
    "import faiss\n",
    "import numpy as np\n",
    "import openai\n",
    "import PyPDF2\n",
    "import requests\n",
    "import spacy\n",
    "import tldextract\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "from pptx import Presentation\n",
    "from tenacity import retry, stop_after_attempt, wait_random_exponential\n",
    "from tkinter import Tk, filedialog\n",
    "from transformers import GPT2Tokenizer\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.utils.data import DataLoader\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "from readppt import read_ppt\n",
    "import urlscrape\n",
    "\n",
    "load_dotenv()\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "MODEL = \"gpt-4\"\n",
    "CHUNK_SIZE=7000\n",
    "\n",
    "# Save the index and dictionaries\n",
    "def save_index_and_paths(index, text_paths, image_paths, index_file, paths_file):\n",
    "    faiss.write_index(index, index_file)\n",
    "    with open(paths_file, \"wb\") as f:\n",
    "        pickle.dump((text_paths, image_paths), f)\n",
    "\n",
    "# Load the index and dictionaries\n",
    "def load_index_and_paths(index_file, paths_file):\n",
    "    index = faiss.read_index(index_file)\n",
    "    with open(paths_file, \"rb\") as f:\n",
    "        text_paths, image_paths = pickle.load(f)\n",
    "    return index, text_paths, image_paths\n",
    "\n",
    "def clean_text(text):\n",
    "    cleaned_text = \" \".join(text.split())\n",
    "    cleaned_text = re.sub(r'http\\S+', '', cleaned_text)\n",
    "    cleaned_text = re.sub(r'<script.*?>.*?</script>', '', cleaned_text, flags=re.DOTALL)\n",
    "    cleaned_text = re.sub(r'<style.*?>.*?</style>', '', cleaned_text, flags=re.DOTALL)\n",
    "    cleaned_text = \" \".join(cleaned_text.split())\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)\n",
    "    cleaned_text = cleaned_text.replace(\"\\n\", \" \").replace(\"\\r\", \" \").replace(\"\\t\", \" \")\n",
    "    cleaned_text = re.sub(r'[^a-zA-Z0-9.,!?/:;()%$@&\\s]', '', cleaned_text)\n",
    "    cleaned_text = re.sub(r'(?i)(terms\\s*and\\s*conditions|privacy\\s*policy|copyright|blog|legal|careers|cdn*).{0,10}', '', cleaned_text)\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)\n",
    "    return cleaned_text\n",
    "\n",
    "def split_text(text: str, max_tokens=CHUNK_SIZE) -> List[str]:\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_tokens = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence_tokens = tokenizer(sentence)[\"input_ids\"]\n",
    "        # Exclude the special tokens ([CLS], [SEP]) from the token count\n",
    "        sentence_token_count = len(sentence_tokens) - 2\n",
    "\n",
    "        if current_tokens + sentence_token_count > max_tokens:\n",
    "            # Exceeds token limit, create a new chunk\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = [sentence]\n",
    "            current_tokens = sentence_token_count\n",
    "        else:\n",
    "            # Append the sentence to the current chunk\n",
    "            current_chunk.append(sentence)\n",
    "            current_tokens += sentence_token_count\n",
    "\n",
    "    # Add the last chunk\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def preprocess_documents(root_folder: str) -> Tuple[List[Tuple[str, str]], List[Tuple[str, PIL.Image.Image]]]:\n",
    "    text_documents = []\n",
    "    image_documents = []\n",
    "\n",
    "    for subdir, dirs, files in os.walk(root_folder):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(subdir, file)\n",
    "            input_type = file.split('.')[-1]\n",
    "\n",
    "            if input_type not in ['pdf', 'pptx', 'jpg', 'png', 'jpeg']:\n",
    "                continue\n",
    "\n",
    "            if input_type in ['pdf', 'pptx']:\n",
    "                text = analyze_input(input_type, None, file_path)\n",
    "                cleaned_text = clean_text(text)\n",
    "                chunks = split_text(cleaned_text)\n",
    "\n",
    "                for chunk in chunks:\n",
    "                    text_documents.append((file_path, chunk))\n",
    "            elif input_type in ['jpg', 'png', 'jpeg']:\n",
    "                image = PIL.Image.open(file_path)\n",
    "                image_documents.append((file_path, image))\n",
    "\n",
    "    return text_documents, image_documents\n",
    "\n",
    "def read_pdf(file):\n",
    "    file.seek(0)  # move the file cursor to the beginning\n",
    "    pdf_reader = PyPDF2.PdfReader(file)\n",
    "    \n",
    "    # Check if the PDF is encrypted\n",
    "    if pdf_reader.is_encrypted:\n",
    "        print(\"Encrypted PDF file detected. Skipping...\")\n",
    "        return \"\"\n",
    "    \n",
    "    if len(pdf_reader.pages) == 0:\n",
    "        raise ValueError(\"PDF file is empty\")\n",
    "    \n",
    "    text = \"\"\n",
    "    for page in pdf_reader.pages:\n",
    "        text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "# For image embedding\n",
    "def embed_image(image: Image.Image) -> np.ndarray:\n",
    "    # Load the pre-trained ResNet model\n",
    "    resnet = models.resnet18(pretrained=True)\n",
    "    resnet.eval()\n",
    "\n",
    "    # Define the preprocessing transformations\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    # Apply the transformations to the image\n",
    "    transformed_image = transform(image).unsqueeze(0)\n",
    "\n",
    "    # Extract the features using the ResNet model\n",
    "    with torch.no_grad():\n",
    "        features = resnet(transformed_image).numpy()\n",
    "\n",
    "    return features\n",
    "\n",
    "# Embed the text\n",
    "def embed_text(text: str) -> np.ndarray:\n",
    "    response = openai.Embedding.create(input=text, model=\"text-embedding-ada-002\")\n",
    "    return np.array(response['data'][0]['embedding'])\n",
    "\n",
    "\n",
    "def index_embeddings(text_documents: List[Tuple[str, str]], image_documents: List[Tuple[str, PIL.Image.Image]]) -> Tuple[faiss.Index, Dict[int, Tuple[str, str]], Dict[int, str]]:\n",
    "    text_paths = {}\n",
    "    image_paths = {}\n",
    "\n",
    "    # Obtain the first text embedding to get its dimensions\n",
    "    first_path, first_text = text_documents[0]\n",
    "    first_embedding = embed_text(first_text)\n",
    "    embedding_dim = first_embedding.shape[0]\n",
    "\n",
    "    # Initialize the index with the dynamic embedding dimension\n",
    "    index = faiss.IndexFlatL2(embedding_dim)\n",
    "\n",
    "    # Add the first text embedding to the index\n",
    "    index.add(first_embedding.reshape(1, -1))\n",
    "    text_paths[0] = (first_path, first_text)  # Store the path and chunk together\n",
    "\n",
    "    current_id = 1\n",
    "\n",
    "    # Add the remaining text embeddings to the index\n",
    "    for path, text in text_documents[1:]:\n",
    "        chunks = split_text(text)\n",
    "        for chunk in chunks:\n",
    "            print(f\"Embedding chunk with {len(chunk)} tokens\")\n",
    "            embedding = embed_text(chunk)\n",
    "            index.add(embedding.reshape(1, -1))\n",
    "            text_paths[current_id] = (path, chunk)  # Store the path and chunk together\n",
    "            current_id += 1\n",
    "\n",
    "    # Add the image embeddings to the index\n",
    "    for path, image in image_documents:\n",
    "        print(\"Embedding image\")\n",
    "        embedding = embed_image(image)\n",
    "        index.add(embedding.reshape(1, -1))\n",
    "        image_paths[current_id] = path\n",
    "        current_id += 1\n",
    "\n",
    "    return index, text_paths, image_paths\n",
    "\n",
    "def search(query: str, index: faiss.IndexIDMap, text_paths: Dict[int, str], image_paths: Dict[int, str]) -> List[Tuple[str, float]]:\n",
    "    # Embed the query\n",
    "    query_embedding = embed_text(query)\n",
    "\n",
    "    # Search the index\n",
    "    D, I = index.search(np.array([query_embedding]), k=10)\n",
    "\n",
    "    # Get the paths and scores of the search results\n",
    "    results = []\n",
    "    for i, score in zip(I[0], D[0]):\n",
    "        if i in text_paths:\n",
    "            path, chunk = text_paths[i]\n",
    "            results.append((path, chunk, score))\n",
    "        elif i in image_paths:\n",
    "            results.append((image_paths[i], \"\", score))  # Add an empty string for the chunk in the case of images\n",
    "\n",
    "    return results\n",
    "\n",
    "# Analyze input\n",
    "def analyze_input(input_type, company, url):\n",
    "    text = \"\"\n",
    "    if input_type == \"url\": # Placeholder for url scraping\n",
    "        data = urlscrape.link(url)\n",
    "    elif input_type in [\"pdf\", \"pptx\"]:\n",
    "        file_path = url\n",
    "\n",
    "        with open(file_path, \"rb\") as file:\n",
    "            if input_type == \"pdf\":\n",
    "                text = read_pdf(file)\n",
    "            elif input_type == \"pptx\":\n",
    "                file_content = file.read()\n",
    "                text = read_ppt(file_content)\n",
    "        data = text\n",
    "    else:\n",
    "        raise ValueError(\"Invalid input type\")\n",
    "\n",
    "    return data\n",
    "\n",
    "def base_gptcall(prompt):\n",
    "    messages = [{\"role\": \"system\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=MODEL,\n",
    "        messages=messages,\n",
    "        temperature=0.1\n",
    "    )\n",
    "    return response.choices[0]['message']['content'].strip()\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=2, max=20), stop=stop_after_attempt(3), reraise=True)\n",
    "def call_gpt(prompt):\n",
    "    answers = []\n",
    "    if len(prompt)>CHUNK_SIZE:\n",
    "        textchunks = split_text(prompt)\n",
    "        for chunk in textchunks:\n",
    "            answer = []\n",
    "            # print(len(chunk))\n",
    "            # print(chunk)\n",
    "            answer = base_gptcall(chunk)\n",
    "            answers.append(answer)\n",
    "        return ' '.join(answers)\n",
    "    else:\n",
    "        return base_gptcall(prompt)\n",
    "\n",
    "def recursive_analyze(text):\n",
    "    text_chunks = clean_text(text)\n",
    "    text_chunks = split_text(text)\n",
    "    print(\"The total length of all text chunks is: \")\n",
    "    print(len(text_chunks))\n",
    "    # Use ThreadPoolExecutor to parallelize GPT calls\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        futures = []\n",
    "        for chunk in text_chunks:\n",
    "            futures.append(executor.submit(call_gpt, f\"Extract all insights, names and facts from the following text as would be useful for an investment memo:\\n\\n{chunk}\"))\n",
    "        insights_lists = [future.result() for future in futures]\n",
    "    combined_insights = \"\\n\".join(insights_lists)\n",
    "    prompt = f\"Please summarise. If no useful information is present, please reply with 'info not available':\\n\\n{combined_insights}\"\n",
    "    summary = call_gpt(prompt)\n",
    "    return summary\n",
    "\n",
    "def main():\n",
    "    openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "    root_folder = '/Users/rohit/Library/CloudStorage/OneDrive-Personal/# Backup/Venture Capital/Decks - funds'\n",
    "    text_documents, image_documents = preprocess_documents(root_folder)\n",
    "    index, text_paths, image_paths = index_embeddings(text_documents, image_documents)\n",
    "\n",
    "    query = \"This investor has held several CSO positions and is a top professional with dealflow.\"\n",
    "    results = search(query, index, text_paths, image_paths)\n",
    "\n",
    "    for path, chunk, score in results:\n",
    "        print(f\"Path: {path}, Chunk: {chunk}, Score: {score}\")\n",
    "\n",
    "    top_result_path, top_result_chunk = results[0][:2]  # Get the path and chunk of the top result\n",
    "    input_type = top_result_path.split('.')[-1]\n",
    "    text = analyze_input(input_type, None, top_result_path)\n",
    "    summary = recursive_analyze(text)\n",
    "\n",
    "    print(\"\\nSummary of the top result:\")\n",
    "    print(summary)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
